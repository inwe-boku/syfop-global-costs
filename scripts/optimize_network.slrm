#!/bin/bash
#
# This file is responsible for one VSC job. The script optimize_network.py can be used to create
# multiple jobs at once. See documentation there for more information.
#
# TODO set parameters there via environment variables? Calculate run time from number of pixels?
#
#SBATCH --job-name          optimize_network
#SBATCH --nodes             1                   # number of nodes
#SBATCH --time              00:45:00            # time limit


##SBATCH --ntasks-per-node   16
##SBATCH --ntasks-per-core   1
##SBATCH --mail-type=BEGIN              # first have to state the type of event to occur
##SBATCH --mail-user=user@example.com   # and then your email address


cd $DATA/syfop-global-costs

eval "$(micromamba shell hook --shell bash --prefix "$HOME/micromamba/" 2> /dev/null)"
micromamba activate syfop

export PYTHONPATH=$PYTHONPATH:$DATA/syfop-global-costs/syfop:$DATA/syfop-global-costs/
export GRB_LICENSE_FILE=$HOME/gurobi.lic
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/gurobi1001/linux64/lib

# TODO should we use ntasks parameter somehow for parallel subprocesses to separate recourses
# better? Or not use so many SLURM jobs in the first place but do it differently? Use the array
# mode by SLURM?
python scripts/optimize_network.py $@
